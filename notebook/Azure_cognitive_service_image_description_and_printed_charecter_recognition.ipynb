{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive services\n",
    "\n",
    "Azure Cognitive Services are APIs, SDKs, and services available to help developers build intelligent applications without having direct AI or data science skills or knowledge. Azure Cognitive Services enable developers to easily add cognitive features into their applications. The goal of Azure Cognitive Services is to help developers create applications that can see, hear, speak, understand, and even begin to reason. The catalog of services within Azure Cognitive Services can be categorized into five main pillars - Vision, Speech, Language, Web Search, and Decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we will have a look at how we can use Azure cognitive service's Computer vision API's to develop your own application with minimal effort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Cognitive Resource\n",
    "\n",
    "To work and interact with Cognitive API's we need to have Cognitive resources. Cognitive resources provides keys and endpoints which we can use to securely transact with our API request and responses. Let us see, how to create a cognitive resource using web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cognitive/step_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our cognitive resource ready to use.\n",
    "<br>\n",
    "\n",
    "![](images/cognitive/step_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigating to the \"Quick Start\" guide will give us good information about what all the things we can do with the cognitive resources and the type of applications which can be built by using the Cognitive services.\n",
    "<br>\n",
    "\n",
    "![](images/cognitive/step_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start using these API's we need to know the \"Key\" and \"Endpoint\" of our cognitive resource, so that we can point our API's to use the same. We can find the \"Key\" and \"Endpoint\" as shown in the image below. Each cognitive resource will have unique key and Endpoint. You should take extra pre-cautions to keep it safe. You can also use the \"Regeneratekey1\" and \"Regeneratekey2\" to reset your \"keys\".\n",
    "\n",
    "<br>\n",
    "\n",
    "![](images/cognitive/step_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Azure cognitive services for computer vision libraries \n",
    "\n",
    "Azure cognitive services provides support for programming languages such as \"Java\", \"C#\", \"Go\", \"Javascript\" and \"Python\". We will be using the Python programming language for the illustrations.\n",
    "We will install the Azure python depndencies for computer vision API's [dependent package](https://pypi.org/project/azure-cognitiveservices-vision-computervision/)\n",
    "\n",
    "We can install the dependencies by runnning the following command `pip install azure-cognitiveservices-vision-computervision==0.5.0`. We have tested this example with version 0.5.0 at the time of writing this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-vision-computervision==0.5.0 in /home/l-8053/py3/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in /home/l-8053/py3/lib/python3.6/site-packages (from azure-cognitiveservices-vision-computervision==0.5.0) (1.1.25)\n",
      "Requirement already satisfied: msrest>=0.5.0 in /home/l-8053/py3/lib/python3.6/site-packages (from azure-cognitiveservices-vision-computervision==0.5.0) (0.6.13)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /home/l-8053/py3/lib/python3.6/site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /home/l-8053/py3/lib/python3.6/site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (0.6.0)\n",
      "Requirement already satisfied: requests~=2.16 in /home/l-8053/py3/lib/python3.6/site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/l-8053/py3/lib/python3.6/site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (2020.4.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/l-8053/py3/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (3.1.0)\n",
      "Requirement already satisfied: six in /home/l-8053/py3/lib/python3.6/site-packages (from isodate>=0.6.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/l-8053/py3/lib/python3.6/site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/l-8053/py3/lib/python3.6/site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/l-8053/py3/lib/python3.6/site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision==0.5.0) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-cognitiveservices-vision-computervision==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required dependencies\n",
    "\n",
    "Now that we have installed the required package, we can start including the dependent Azure modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import TextOperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import TextRecognitionMode\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "#from array import array\n",
    "import os\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:Set the COMPUTER_VISION_SUBSCRIPTION_KEY environment variable.\n",
      "ERROR:Set the COMPUTER_VISION_ENDPOINT environment variable.\n"
     ]
    }
   ],
   "source": [
    "# Add your Computer Vision subscription key to your environment variables.\n",
    "if 'COMPUTER_VISION_SUBSCRIPTION_KEY' in os.environ:\n",
    "    subscription_key = os.environ['COMPUTER_VISION_SUBSCRIPTION_KEY']\n",
    "else:\n",
    "    print(\"ERROR:Set the COMPUTER_VISION_SUBSCRIPTION_KEY environment variable.\")\n",
    "\n",
    "# Add your Computer Vision endpoint to your environment variables.\n",
    "if 'COMPUTER_VISION_ENDPOINT' in os.environ:\n",
    "    endpoint = os.environ['COMPUTER_VISION_ENDPOINT']\n",
    "else:\n",
    "    print(\"ERROR:Set the COMPUTER_VISION_ENDPOINT environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the environment variables\n",
    "\n",
    "To use the Cognitive services we need to link our Cognitive services by exporting the environment variables. For Computer vision services we need to set following two environment variables.\n",
    "\n",
    "* COMPUTER_VISION_SUBSCRIPTION_KEY\n",
    "* COMPUTER_VISION_ENDPOINT\n",
    "\n",
    "If you remember from our previous step we have noted the \"Key\" and \"Endpoint\" as a final step of cognitive resource creation step. We need to use the same. Since we are using the notebook environment, we will use the jupyter notebook way of exporting variables as shown below.\n",
    "\n",
    "* `%env COMPUTER_VISION_SUBSCRIPTION_KEY Your_Key_value`\n",
    "* `%env COMPUTER_VISION_ENDPOINT your_end_point`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COMPUTER_VISION_SUBSCRIPTION_KEY=5f906a706cbf49bca5de9775901e6744\n"
     ]
    }
   ],
   "source": [
    "%env COMPUTER_VISION_SUBSCRIPTION_KEY 5f906a706cbf49bca5de9775901e6744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COMPUTER_VISION_ENDPOINT=https://westeurope.api.cognitive.microsoft.com/\n"
     ]
    }
   ],
   "source": [
    "%env COMPUTER_VISION_ENDPOINT https://westeurope.api.cognitive.microsoft.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your Computer Vision subscription key to your environment variables.\n",
    "if 'COMPUTER_VISION_SUBSCRIPTION_KEY' in os.environ:\n",
    "    subscription_key = os.environ['COMPUTER_VISION_SUBSCRIPTION_KEY']\n",
    "else:\n",
    "    print(\"ERROR:Set the COMPUTER_VISION_SUBSCRIPTION_KEY environment variable.\")\n",
    "\n",
    "# Add your Computer Vision endpoint to your environment variables.\n",
    "if 'COMPUTER_VISION_ENDPOINT' in os.environ:\n",
    "    endpoint = os.environ['COMPUTER_VISION_ENDPOINT']\n",
    "else:\n",
    "    print(\"ERROR:Set the COMPUTER_VISION_ENDPOINT environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our environment variables are set and hence we don't observe any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a client to interact with the Congnitive service server\n",
    "\n",
    "We need to create a client which will interact with the Cognitive server. We will pass in our keys which we obtained from the environment variables.\n",
    "\n",
    "It is not mandatory to export the environment variables which we created in the previous step in this example. We can pass the key and endpoint directly. But when you try to deploy it as a complete solution, it is important to secure your key and endpoint information. Usually as a best practice, it is secured by using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Image Description API\n",
    "\n",
    "Let us use the API's provided by Cognitive services to get the description of any given image. i.e we provide an image URL from internet to the service as an input and Cognitive service will respond back with a description of the given image. It also informs about the confidence level. Let us see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remote_image_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/landmark.jpg\"\n",
    "#remote_image_url = \"https://cdn.getyourguide.com/img/tour_img-1543839-148.jpg\"\n",
    "remote_image_url = \"https://i.pinimg.com/originals/2b/87/ff/2b87ffcbf89d763a01b4b05e2e447dd8.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our input image for testing purposes. \n",
    "<br>\n",
    "\n",
    "![](https://i.pinimg.com/originals/2b/87/ff/2b87ffcbf89d763a01b4b05e2e447dd8.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of remote image: \n",
      "'a group of people standing in front of a crowd posing for the camera' with confidence 94.21%\n"
     ]
    }
   ],
   "source": [
    "# Call API\n",
    "description_results = computervision_client.describe_image(remote_image_url )\n",
    "\n",
    "# Get the captions (descriptions) from the response, with confidence level\n",
    "print(\"Description of remote image: \")\n",
    "if (len(description_results.captions) == 0):\n",
    "    print(\"No description detected.\")\n",
    "else:\n",
    "    for caption in description_results.captions:\n",
    "        print(\"'{}' with confidence {:.2f}%\".format(caption.text, caption.confidence * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the Image descriptor\n",
    "\n",
    "**'a group of people standing in front of a crowd posing for the camera' with confidence 94.21%**\n",
    "\n",
    "In deed! House of Stark, waiting for the arrival of King \"Robert Baratheon\" and posing for the camera of course! :)\n",
    "\n",
    "This is quite fun. Without knowing anything about the image processing or Computer vision or Artificial intelligence. We were able to build an application using the Azure cognitive services. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printed text recognizer\n",
    "\n",
    "Let us try out one more cognitive service API. The printed text recognizer recognizes printed text in a given image. \n",
    "This example will extract printed text in an image, then print results, line by line.\n",
    "This API call can also recognize handwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image with printed text\n",
    "remote_image_printed_text_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/printed_text.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the sample image we are giving as input.\n",
    "<br>\n",
    "\n",
    "![](https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/printed_text.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the printed text from an image is a two step process.\n",
    "* Step 1: Provide an input image with printed text. As a response API provides a resonse where the extracted result has been stored as an operation ID.\n",
    "* Step 2: Extract the operation ID obtained in Step-1 and extract the actual text from the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call API with URL and raw response (allows you to get the operation location)\n",
    "recognize_printed_results = computervision_client.batch_read_file(remote_image_printed_text_url,  raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Operation-Location': 'https://westeurope.api.cognitive.microsoft.com/vision/v2.1/read/operations/40704246-91e4-452e-aebf-3ee5ed501a44'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_printed_results.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutrition Facts Amount Per Serving\n",
      "[144.0, 0.0, 1238.0, 211.0, 1224.0, 280.0, 130.0, 57.0]\n",
      "Serving size: 1 bar (40g)\n",
      "[110.0, 58.0, 598.0, 157.0, 587.0, 206.0, 100.0, 108.0]\n",
      "Serving Per Package: 4\n",
      "[83.0, 108.0, 548.0, 206.0, 538.0, 256.0, 72.0, 157.0]\n",
      "Total Fat 13g\n",
      "[683.0, 213.0, 1000.0, 286.0, 989.0, 332.0, 672.0, 260.0]\n",
      "Saturated Fat 1.5g\n",
      "[695.0, 295.0, 1120.0, 394.0, 1108.0, 447.0, 683.0, 347.0]\n",
      "Amount Per Serving\n",
      "[29.0, 207.0, 491.0, 309.0, 478.0, 367.0, 16.0, 265.0]\n",
      "Trans Fat 0g\n",
      "[668.0, 363.0, 954.0, 435.0, 940.0, 488.0, 655.0, 416.0]\n",
      "alories 190\n",
      "[8.0, 293.0, 265.0, 350.0, 254.0, 396.0, 0.0, 339.0]\n",
      "Cholesterol Omg\n",
      "[593.0, 424.0, 1007.0, 526.0, 993.0, 580.0, 579.0, 479.0]\n",
      "ories from Fat 110\n",
      "[9.0, 377.0, 398.0, 464.0, 388.0, 509.0, 0.0, 421.0]\n",
      "Sodium 20mg\n",
      "[561.0, 497.0, 913.0, 588.0, 899.0, 643.0, 547.0, 552.0]\n",
      "nt Daily Values are based on\n",
      "[7.0, 476.0, 521.0, 598.0, 511.0, 640.0, 0.0, 518.0]\n",
      "Vitamin A 50%\n",
      "[525.0, 597.0, 776.0, 657.0, 766.0, 699.0, 514.0, 640.0]\n",
      "calorie diet.\n",
      "[12.0, 534.0, 196.0, 576.0, 187.0, 615.0, 4.0, 574.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the operation location (URL with an ID at the end) from the response\n",
    "operation_location_remote = recognize_printed_results.headers[\"Operation-Location\"]\n",
    "# Grab the ID from the URL\n",
    "operation_id = operation_location_remote.split(\"/\")[-1]\n",
    "\n",
    "# Call the \"GET\" API and wait for it to retrieve the results \n",
    "while True:\n",
    "    get_printed_text_results = computervision_client.get_read_operation_result(operation_id)\n",
    "    if get_printed_text_results.status not in ['NotStarted', 'Running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the detected text, line by line\n",
    "if get_printed_text_results.status == TextOperationStatusCodes.succeeded:\n",
    "    for text_result in get_printed_text_results.recognition_results:\n",
    "        for line in text_result.lines:\n",
    "            print(line.text)\n",
    "            print(line.bounding_box)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe we were able to extract the printed text from an image. Go ahead and build your own nutrition monitor app!. Just scan the food package boxes, monitor the amount of carbohydrates, fat and sugar you consume. \n",
    "\n",
    "We can also pipe the output of our text scanner to Azure Text analytics or Text translators to create your own complete application of Text translator for different languages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see Azure cognitive services will help you develop a complete solution without having to worry too much about the internals of the AI or ML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
